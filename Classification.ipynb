{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification is the case of **Supervised Learning** where the target is always given. The explicit examples of output is provided what target model is supposed to produce for specific input. The data is split into an input space ${X}$ and an output space  ${Y}$\n",
    "\n",
    "$$\\Large{f:X->Y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this classifcation demo we are going to identify the genre of the text by using:\n",
    "    - Statistical Models\n",
    "        - Naive Bayes\n",
    "        - SVM \n",
    "    - Neural Network Models\n",
    "        - BiLSTM\n",
    "    - Pretrained Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have the news data. I am not sure from where I got this dataset but I don't claim this is my property or I created it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from TextPreProcessing import Preprocessing\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, string\n",
    "import spacy, pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Loading model\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "business         610\n",
       "entertainment    486\n",
       "food              59\n",
       "graphics          65\n",
       "historical        22\n",
       "politics         517\n",
       "sport            298\n",
       "tech             413\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data using our favourite library pandas :D \n",
    "df = pd.read_csv(\"./train.csv\", sep=',',  encoding='utf8')\n",
    "# to see how much data we have on each class \n",
    "df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing news and save them in new column\n",
    "preprocess = Preprocessing()\n",
    "df['cleanText'] = df['text'].apply(preprocess.normalizeText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
       "      <td>business</td>\n",
       "      <td>ad sales boost time warner profit quarterly pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n",
       "      <td>business</td>\n",
       "      <td>dollar gains on greenspan speech the dollar ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n",
       "      <td>business</td>\n",
       "      <td>yukos unit buyer faces loan claim the owners o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n",
       "      <td>business</td>\n",
       "      <td>high fuel prices hit ba s profits british airw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n",
       "      <td>business</td>\n",
       "      <td>pernod takeover talk lifts domecq shares in uk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  \\\n",
       "0  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...  business   \n",
       "1  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...  business   \n",
       "2  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...  business   \n",
       "3  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...  business   \n",
       "4  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...  business   \n",
       "\n",
       "                                           cleanText  \n",
       "0  ad sales boost time warner profit quarterly pr...  \n",
       "1  dollar gains on greenspan speech the dollar ha...  \n",
       "2  yukos unit buyer faces loan claim the owners o...  \n",
       "3  high fuel prices hit ba s profits british airw...  \n",
       "4  pernod takeover talk lifts domecq shares in uk...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[business, entertainment, politics, sport, tech, food, graphics, historical]\n",
       "Categories (8, object): [business, entertainment, politics, sport, tech, food, graphics, historical]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the labels into catgories (like 'business' -> 1, entertainment -> 2 etc) save them in new column labelCategory\n",
    "\n",
    "df['label'] = df['label'].astype('category')\n",
    "df['labelCategory'] = df['label'].cat.codes\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test split\n",
    "\n",
    "### This function from scikit-learn will randomly split the data into test and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "trainX, testX, trainY, testY = model_selection.train_test_split(df['cleanText'], df['label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n",
    "transform text data into feature vector. Features can be get using\n",
    "\n",
    "- Counter Vector or Bag of Words: converting text to matrix \n",
    "- TF-IDF Vector\n",
    "     - Word level\n",
    "     - Character level\n",
    "     - N-Gram level\n",
    "- Word embeddings\n",
    "- Text/NLP based features\n",
    "- Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "\n",
    "Scikit-learnâ€™s CountVectorizer converts the text documents into count vector form.The strings are converted in tokens first then it places 1 on each word in the document and increases its count if it appears again in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618, 29336) (1852, 29336) (1852,)\n"
     ]
    }
   ],
   "source": [
    "# 1- Counter Vector\n",
    "countVec = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "countVec.fit(df['text'].apply(lambda x: np.str_(x)))\n",
    "\n",
    "# transform the training and testing data using count vectorize object \n",
    "trainXCount = countVec.transform(trainX.apply(lambda x: np.str_(x)))\n",
    "testXCount = countVec.transform(testX.apply(lambda x: np.str_(x)))\n",
    "\n",
    "# to see the features countVec.get_feature_names()\n",
    "print(testXCount.shape, trainXCount.shape, trainY.shape)\n",
    "\n",
    "# saving countVector to pickle file\n",
    "pickle.dump(countVec.vocabulary_, open(\"countVector.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequence Inverse Term Frequence (TF-IDF)\n",
    "TF-IDF stands for term frequency-inverse document frequency. The higher the value of TF-IDF for a term, the higher its frequency in the corpus and low document frequency will be. In other words, to get the higher value means a word is rear in the whole document but frequent in a document.\n",
    "\n",
    "Tf-IDF can be performed on:\n",
    "    - Character level\n",
    "    - Word level\n",
    "    - N-gram level\n",
    "\n",
    "For this demo we are going to perfom on all the three to see and compare the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level\n",
    "tfIDF = TfidfVectorizer(analyzer=\"word\", token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfIDF.fit(df['cleanText'].apply(lambda x: np.str_(x)))\n",
    "trainX_TfIDF = tfIDF.transform(trainX.apply(lambda x: np.str_(x)))\n",
    "testX_TfIDF = tfIDF.transform(testX.apply(lambda x: np.str_(x)))\n",
    "pickle.dump(tfIDF, open(\"tfIDFWord.pkl\", \"wb\"))\n",
    "\n",
    "# ngram level \n",
    "tfIDF = TfidfVectorizer(token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "\n",
    "tfIDF.fit(df['cleanText'].apply(lambda x: np.str_(x)))\n",
    "trainX_TfIDFNgram = tfIDF.fit_transform(trainX.apply(lambda x: np.str_(x)))\n",
    "testX_TfIDFNgram = tfIDF.fit_transform(testX.apply(lambda x: np.str_(x)))\n",
    "pickle.dump(tfIDF, open(\"tfIDFNGram.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vector level\n",
    "tfIDF = TfidfTransformer()\n",
    "trainX_TfCount = tfIDF.fit_transform(trainXCount)\n",
    "testX_TfIDFCount = tfIDF.fit_transform(testXCount)\n",
    "pickle.dump(tfIDF, open(\"tfIDFCount.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTraining(model, trainX, trainY, testX):\n",
    "    \"\"\" \n",
    "        trainX = vectorized data\n",
    "        trainY = labels of 'vectorized data'\n",
    "        testY = test data \n",
    "    \"\"\"\n",
    "    classifier = model.fit(trainX, trainY)\n",
    "#     pickle.dump(model, open(\"modelMNB_VecCount_TfIDF.pkl\", \"wb\"))\n",
    "    pickle.dump(model, open(\"modelSVM_TfIDF.pkl\", \"wb\"))\n",
    "    predictions = classifier.predict(testX)\n",
    "    print(metrics.confusion_matrix(testY, predictions))\n",
    "    print(metrics.accuracy_score(predictions, testY) * 100)\n",
    "    print( metrics.classification_report(testY, predictions, target_names=  ['business', 'entertainment', 'politics', 'sport', 'tech', 'food', 'graphics', 'historical']))\n",
    "    return metrics.accuracy_score(predictions, testY) * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145   0   0   0   0   1   0   0]\n",
      " [  2 111   0   0   0   3   0   0]\n",
      " [ 13   0   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   5   0  10]\n",
      " [  4   0   0   0   0   3   0   0]\n",
      " [  2   0   0   0   0 134   0   0]\n",
      " [  3   1   0   0   0   7  53   0]\n",
      " [  7   2   0   0   0   6   0 103]]\n",
      "88.83495145631069\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.82      0.99      0.90       146\n",
      "entertainment       0.97      0.96      0.97       116\n",
      "     politics       1.00      0.19      0.32        16\n",
      "        sport       0.00      0.00      0.00        15\n",
      "         tech       0.00      0.00      0.00         7\n",
      "         food       0.84      0.99      0.91       136\n",
      "     graphics       1.00      0.83      0.91        64\n",
      "   historical       0.91      0.87      0.89       118\n",
      "\n",
      "     accuracy                           0.89       618\n",
      "    macro avg       0.69      0.60      0.61       618\n",
      " weighted avg       0.87      0.89      0.87       618\n",
      "\n",
      "Naive Bayes accuracy-> tf-IDF Word level =  88.83495145631069\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# 1- vectorCount => 49%\n",
    "# accuracy = modelTraining(MultinomialNB(), trainXCount, trainY, testXCount) \n",
    "# print(\"Naive Bayes accuracy -> vectorCount = \", accuracy)\n",
    "# 2- tf-IDF Word level => 83.44\n",
    "# accuracy = modelTraining(MultinomialNB(), trainX_TfIDF, trainY, testX_TfIDF)\n",
    "# print(\"Naive Bayes accuracy-> tf-IDF Word level = \", accuracy)\n",
    "# # 3- tf-IDF n-gram uni and bigram level => 86.08, bi and trigram => 82.67\n",
    "# accuracy = modelTraining(MultinomialNB(), trainX_TfIDFNgram, trainY, testX_TfIDFNgram)\n",
    "# print(\"Naive Bayes accuracy-> tf-IDF n-gram = \", accuracy)\n",
    "# # 4- tf-IDF Vector Count transformer level => 87.05\n",
    "accuracy = modelTraining(MultinomialNB(), trainX_TfCount, trainY, testX_TfIDFCount)\n",
    "print(\"Naive Bayes accuracy-> tf-IDF Word level = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145   0   0   0   0   0   0   1]\n",
      " [  1 114   0   0   0   1   0   0]\n",
      " [  2   0  14   0   0   0   0   0]\n",
      " [  4   0   0  11   0   0   0   0]\n",
      " [  4   2   0   0   0   1   0   0]\n",
      " [  5   0   0   0   0 130   0   1]\n",
      " [  2   0   0   0   0   0  62   0]\n",
      " [  2   3   0   0   0   0   0 113]]\n",
      "95.30744336569579\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.88      0.99      0.93       146\n",
      "entertainment       0.96      0.98      0.97       116\n",
      "     politics       1.00      0.88      0.93        16\n",
      "        sport       1.00      0.73      0.85        15\n",
      "         tech       0.00      0.00      0.00         7\n",
      "         food       0.98      0.96      0.97       136\n",
      "     graphics       1.00      0.97      0.98        64\n",
      "   historical       0.98      0.96      0.97       118\n",
      "\n",
      "     accuracy                           0.95       618\n",
      "    macro avg       0.85      0.81      0.83       618\n",
      " weighted avg       0.95      0.95      0.95       618\n",
      "\n",
      "SVM accuracy-> tf-IDF n-gram =  95.30744336569579\n"
     ]
    }
   ],
   "source": [
    "# 4- SVM => 50.16\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)\n",
    "accuracy = modelTraining(svm.SVC(), trainX_TfCount, trainY, testX_TfIDFCount)\n",
    "print(\"SVM accuracy-> tf-IDF n-gram = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import model_selection, preprocessing, linear_model, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = model_selection.train_test_split(df['cleanText'], df['labelCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'business', 1: 'entertainment', 2: 'food', 3: 'graphics', 4: 'historical', 5: 'politics', 6: 'sport', 7: 'tech'}\n"
     ]
    }
   ],
   "source": [
    "# we are using wandb library \n",
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.wandb_project = 'newsClassification'\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.01\n",
    "model_args.early_stopping_metric = \"mcc\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 5\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(trainX)\n",
    "\n",
    "train_df['label'] = pd.DataFrame(trainY)\n",
    "labelsdict = dict(enumerate(df['label'].cat.categories))\n",
    "print(labelsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# fine-tune RoBERTa based model for classification\n",
    "model = ClassificationModel(\n",
    "     \"roberta\", \n",
    "    # './trained-models/transformer-small/',  \n",
    "    \"roberta-base\",  \n",
    "    num_labels=len(labelsdict),\n",
    "    use_cuda=False,\n",
    "    args=model_args\n",
    ") \n",
    "# for training\n",
    "model.train_model(train_df, acc=sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0161173db775b1ffc607617dcd93ca911c945389657c068f66f111a02f753df6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
